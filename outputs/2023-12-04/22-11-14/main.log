[2023-12-04 22:11:14,742][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2023-12-04 22:11:19,506][flwr][INFO] - Flower VCE: Ray initialized with resources: {'object_store_memory': 59576632934.0, 'node:128.105.144.67': 1.0, 'GPU': 1.0, 'memory': 129012143514.0, 'accelerator_type:P100': 1.0, 'CPU': 40.0}
[2023-12-04 22:11:19,508][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2023-12-04 22:11:19,508][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 0.2}
[2023-12-04 22:11:19,550][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 5 actors
[2023-12-04 22:11:19,551][flwr][INFO] - Initializing global parameters
[2023-12-04 22:11:19,551][flwr][INFO] - Requesting initial parameters from one random client
[2023-12-04 22:11:24,928][flwr][INFO] - Received initial parameters from one random client
[2023-12-04 22:11:24,929][flwr][INFO] - Evaluating initial parameters
[2023-12-04 22:11:27,789][flwr][ERROR] - Given groups=1, weight of size [16, 3, 3, 3], expected input[10, 1, 28, 28] to have 3 channels, but got 1 channels instead
[2023-12-04 22:11:27,793][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 163, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/users/wang2451/Fed_with_flower/utils.py", line 137, in evaluate
    loss, accuracy = model.test(net, testloader, device=device)
  File "/users/wang2451/Fed_with_flower/model.py", line 142, in test
    outputs = net(images)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward
    x = self.conv1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [16, 3, 3, 3], expected input[10, 1, 28, 28] to have 3 channels, but got 1 channels instead

[2023-12-04 22:11:27,794][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 4, 'num_gpus': 0.2} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 4, 'num_gpus': 0.2}.
