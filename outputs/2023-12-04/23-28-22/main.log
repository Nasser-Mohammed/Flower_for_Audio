[2023-12-04 23:28:24,275][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2023-12-04 23:28:28,139][flwr][INFO] - Flower VCE: Ray initialized with resources: {'node:128.105.144.67': 1.0, 'CPU': 40.0, 'GPU': 1.0, 'object_store_memory': 59522786918.0, 'accelerator_type:P100': 1.0, 'memory': 128886502810.0}
[2023-12-04 23:28:28,140][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2023-12-04 23:28:28,140][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 0.2}
[2023-12-04 23:28:28,163][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 5 actors
[2023-12-04 23:28:28,164][flwr][INFO] - Initializing global parameters
[2023-12-04 23:28:28,164][flwr][INFO] - Requesting initial parameters from one random client
[2023-12-04 23:28:36,331][flwr][INFO] - Received initial parameters from one random client
[2023-12-04 23:28:36,331][flwr][INFO] - Evaluating initial parameters
[2023-12-04 23:28:43,094][flwr][ERROR] - "nll_loss_forward_reduce_cuda_kernel_2d_index" not implemented for 'Float'
[2023-12-04 23:28:43,097][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 163, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/users/wang2451/Fed_with_flower/utils.py", line 137, in evaluate
    loss, accuracy = model.test(net, testloader, device=device)
  File "/users/wang2451/Fed_with_flower/model.py", line 147, in test
    loss += criterion(outputs, labels).item()
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/functional.py", line 3053, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: "nll_loss_forward_reduce_cuda_kernel_2d_index" not implemented for 'Float'

[2023-12-04 23:28:43,097][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 4, 'num_gpus': 0.2} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 4, 'num_gpus': 0.2}.
