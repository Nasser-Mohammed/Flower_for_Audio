[2023-12-05 21:45:41,233][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=50, round_timeout=None)
[2023-12-05 21:45:46,123][flwr][INFO] - Flower VCE: Ray initialized with resources: {'accelerator_type:P100': 1.0, 'GPU': 1.0, 'memory': 122336069018.0, 'node:128.105.144.67': 1.0, 'CPU': 40.0, 'object_store_memory': 56715458150.0}
[2023-12-05 21:45:46,124][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2023-12-05 21:45:46,125][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 0.2}
[2023-12-05 21:45:46,162][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 5 actors
[2023-12-05 21:45:46,163][flwr][INFO] - Initializing global parameters
[2023-12-05 21:45:46,163][flwr][INFO] - Requesting initial parameters from one random client
[2023-12-05 21:45:54,584][flwr][INFO] - Received initial parameters from one random client
[2023-12-05 21:45:54,585][flwr][INFO] - Evaluating initial parameters
[2023-12-05 21:46:10,312][flwr][INFO] - initial parameters (loss, other metrics): 696523340.1856, {'accuracy': 0.1}
[2023-12-05 21:46:10,312][flwr][INFO] - FL starting
[2023-12-05 21:46:10,312][flwr][DEBUG] - fit_round 1: strategy sampled 10 clients (out of 10)
[2023-12-05 21:46:37,259][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 7 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:46:37,260][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 7 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:46:37,331][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 3 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:46:37,332][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 3 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:46:41,010][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward
    x = self.conv1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 7.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 262.00 MiB memory in use. Of the allocated memory 7.06 MiB is allocated by PyTorch, and 962.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward\n    x = self.conv1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 7.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 262.00 MiB memory in use. Of the allocated memory 7.06 MiB is allocated by PyTorch, and 962.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2023-12-05 21:46:44,572][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward
    x = self.conv1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 7.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 262.00 MiB memory in use. Of the allocated memory 7.06 MiB is allocated by PyTorch, and 962.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward\n    x = self.conv1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 7.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 262.00 MiB memory in use. Of the allocated memory 7.06 MiB is allocated by PyTorch, and 962.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2023-12-05 21:46:44,611][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 2 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:46:48,136][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 2 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:46:48,178][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 6 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:46:48,180][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 6 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:46:48,180][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:46:48,181][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:46:48,539][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 4 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:46:48,540][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 4 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:46:56,312][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward
    x = self.conv1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 7.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 262.00 MiB memory in use. Of the allocated memory 7.06 MiB is allocated by PyTorch, and 962.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 9 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward\n    x = self.conv1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 7.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 262.00 MiB memory in use. Of the allocated memory 7.06 MiB is allocated by PyTorch, and 962.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2023-12-05 21:46:56,313][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward
    x = self.conv1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 7.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 262.00 MiB memory in use. Of the allocated memory 7.06 MiB is allocated by PyTorch, and 962.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 9 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 101, in forward\n    x = self.conv1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 7.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.38 GiB memory in use. Including non-PyTorch memory, this process has 262.00 MiB memory in use. Of the allocated memory 7.06 MiB is allocated by PyTorch, and 962.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2023-12-05 21:49:53,294][flwr][DEBUG] - fit_round 1 received 2 results and 8 failures
[2023-12-05 21:49:53,440][flwr][WARNING] - No fit_metrics_aggregation_fn provided
[2023-12-05 21:50:14,337][flwr][INFO] - fit progress: (1, 2569846480.0768, {'accuracy': 0.1}, 244.02487540792208)
[2023-12-05 21:50:14,338][flwr][INFO] - evaluate_round 1: no clients selected, cancel
[2023-12-05 21:50:14,338][flwr][DEBUG] - fit_round 2: strategy sampled 10 clients (out of 10)
[2023-12-05 21:50:36,971][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 6 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:50:36,972][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 6 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:50:40,779][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:50:44,164][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:50:44,399][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 5 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2023-12-05 21:50:44,399][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 5 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2023-12-05 21:50:51,632][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:50:51,653][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:50:51,685][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 9 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:50:51,685][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 9 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:50:51,688][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 4 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:50:51,689][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 4 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:50:52,035][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 7 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:50:52,035][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 7 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:51:00,355][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 2 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2023-12-05 21:51:00,356][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 2 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2023-12-05 21:53:48,040][flwr][DEBUG] - fit_round 2 received 2 results and 8 failures
[2023-12-05 21:53:57,316][flwr][INFO] - fit progress: (2, 2770031007.3344, {'accuracy': 0.1}, 467.0037758349208)
[2023-12-05 21:53:57,316][flwr][INFO] - evaluate_round 2: no clients selected, cancel
[2023-12-05 21:53:57,317][flwr][DEBUG] - fit_round 3: strategy sampled 10 clients (out of 10)
[2023-12-05 21:54:19,722][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 7 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2023-12-05 21:54:23,352][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 7 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2023-12-05 21:54:27,109][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 5 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:54:27,111][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 5 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:54:27,112][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:54:27,113][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 0 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:54:30,768][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 4 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2023-12-05 21:54:30,768][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 4 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2023-12-05 21:54:34,412][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 6 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:54:34,414][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 6 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:54:34,416][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 9 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2023-12-05 21:54:34,416][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:54:34,417][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit
    model.train(
  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train
    net = _training_loop(net, trainloader, device, criterion, optimizer)
  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop
    loss = criterion(net(images), labels)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward
    x = self.layer1(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward
    shortcut = self.down_sample(x)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717571, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9a5438b220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 9 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/users/wang2451/Fed_with_flower/client.py", line 60, in fit\n    model.train(\n  File "/users/wang2451/Fed_with_flower/model.py", line 78, in train\n    net = _training_loop(net, trainloader, device, criterion, optimizer)\n  File "/users/wang2451/Fed_with_flower/model.py", line 112, in _training_loop\n    loss = criterion(net(images), labels)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 104, in forward\n    x = self.layer1(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/container.py", line 215, in forward\n    input = module(input)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 42, in forward\n    shortcut = self.down_sample(x)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File "/users/wang2451/Fed_with_flower/src/model.py", line 19, in forward\n    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 11.91 GiB of which 5.38 MiB is free. Process 708842 has 980.00 MiB memory in use. Process 712767 has 1.39 GiB memory in use. Process 712766 has 1.39 GiB memory in use. Process 712764 has 1.39 GiB memory in use. Process 712763 has 1.39 GiB memory in use. Process 712761 has 1.39 GiB memory in use. Process 713609 has 980.00 MiB memory in use. Process 717573 has 1.39 GiB memory in use. Process 717572 has 1.21 GiB memory in use. Including non-PyTorch memory, this process has 444.00 MiB memory in use. Of the allocated memory 132.08 MiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2023-12-05 21:54:34,417][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717569, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fdb1c7471c0>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 1 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2023-12-05 21:54:34,933][flwr][ERROR] - Traceback (most recent call last):
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/ray/_private/worker.py", line 2380, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 3 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2023-12-05 21:54:34,934][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn
    net = pyramidnet().to(device)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=717570, ip=128.105.144.67, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f0a4c733220>)
  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 3 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 70, in run\n    client = check_clientfn_returns_client(client_fn(cid))\n  File "/users/wang2451/Fed_with_flower/client.py", line 126, in client_fn\n    net = pyramidnet().to(device)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to\n    return self._apply(convert)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply\n    module._apply(fn)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply\n    param_applied = fn(param)\n  File "/users/wang2451/miniconda3/envs/flower/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
